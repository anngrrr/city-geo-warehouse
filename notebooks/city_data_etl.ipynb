{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ef7c8",
   "metadata": {},
   "source": [
    "# City Geographic Data ETL Project\n",
    "\n",
    "This notebook demonstrates the ETL (Extract, Transform, Load) process for city geographic and quality-of-life data. We'll cover:\n",
    "- Setting up database connections\n",
    "- Loading and cleaning raw data\n",
    "- Transforming and validating data\n",
    "- Loading data into PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4843446",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import required libraries and configure our database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from src.models.city_data_collector import CityDataCollector\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize data collector\n",
    "city_collector = CityDataCollector()\n",
    "\n",
    "# Database connection with error handling\n",
    "try:\n",
    "    DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "    if not DATABASE_URL:\n",
    "        raise ValueError(\"DATABASE_URL environment variable is not set\")\n",
    "        \n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"SELECT 1\"))\n",
    "    logger.info(\"Successfully connected to the database\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Database connection error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c3517",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data\n",
    "\n",
    "Let's create some sample city data to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652312aa",
   "metadata": {},
   "source": [
    "## 2. Collect Real City Data\n",
    "\n",
    "Now we'll collect real data from various APIs:\n",
    "- Basic city information from World Cities Database\n",
    "- Quality of life metrics from Numbeo\n",
    "- Environmental data from OpenWeatherMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ETL process\n",
    "from src.models.city_etl import CityDataETL\n",
    "\n",
    "etl = CityDataETL(DATABASE_URL)\n",
    "\n",
    "# List of cities to process\n",
    "cities = [\n",
    "    {'name': 'New York', 'country': 'US'},\n",
    "    {'name': 'London', 'country': 'GB'},\n",
    "    {'name': 'Tokyo', 'country': 'JP'},\n",
    "    {'name': 'Paris', 'country': 'FR'},\n",
    "    {'name': 'Singapore', 'country': 'SG'},\n",
    "    {'name': 'Sydney', 'country': 'AU'},\n",
    "    {'name': 'Dubai', 'country': 'AE'},\n",
    "    {'name': 'Moscow', 'country': 'RU'},\n",
    "    {'name': 'Berlin', 'country': 'DE'},\n",
    "    {'name': 'Toronto', 'country': 'CA'},\n",
    "    {'name': 'Barcelona', 'country': 'ES'},\n",
    "    {'name': 'Amsterdam', 'country': 'NL'},\n",
    "    {'name': 'Seoul', 'country': 'KR'},\n",
    "    {'name': 'Mumbai', 'country': 'IN'},\n",
    "    {'name': 'Sao Paulo', 'country': 'BR'}\n",
    "]\n",
    "\n",
    "# Run ETL process\n",
    "df = etl.run_etl(cities)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nETL Process Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total cities processed: {len(cities)}\")\n",
    "print(f\"Successfully collected data for {len(df)} cities\")\n",
    "print(\"\\nColumns with data:\")\n",
    "for column in df.columns:\n",
    "    non_null = df[column].count()\n",
    "    print(f\"{column}: {non_null} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c032c9",
   "metadata": {},
   "source": [
    "## 3. Analyze and Validate Collected Data\n",
    "\n",
    "Let's analyze the data we collected to ensure quality and completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39884d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data quality checks\n",
    "def analyze_data_quality(df):\n",
    "    print(\"Data Quality Report:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(\"\\nMissing values by column:\")\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    # Check numeric columns statistics\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(\"\\nNumeric columns statistics:\")\n",
    "    print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Check for potential outliers\n",
    "    print(\"\\nPotential outliers (values beyond 3 standard deviations):\")\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = df[(df[col] > mean + 3*std) | (df[col] < mean - 3*std)][col]\n",
    "        if not outliers.empty:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(outliers)\n",
    "\n",
    "# Analyze the collected data\n",
    "analyze_data_quality(df_real)\n",
    "\n",
    "# Visualize some key metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_real[[\n",
    "    'happiness_index', \n",
    "    'health_index', \n",
    "    'cost_of_living_index', \n",
    "    'education_level_score'\n",
    "]])\n",
    "plt.title('Distribution of Key Quality of Life Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27034d57",
   "metadata": {},
   "source": [
    "## 4. Natural Features Analysis\n",
    "\n",
    "Let's analyze the natural features data we collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze natural features data\n",
    "natural_features_cols = [\n",
    "    'distance_to_water',\n",
    "    'distance_to_mountains',\n",
    "    'distance_to_forest',\n",
    "    'distance_to_park',\n",
    "    'forest_proximity_score',\n",
    "    'green_space_ratio'\n",
    "]\n",
    "\n",
    "print(\"Natural Features Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nAverage distances (km):\")\n",
    "for col in natural_features_cols:\n",
    "    if 'distance' in col:\n",
    "        mean_val = df_real[col].mean()\n",
    "        if not pd.isna(mean_val):\n",
    "            print(f\"{col}: {mean_val:.2f}\")\n",
    "\n",
    "print(\"\\nGreen Space Metrics:\")\n",
    "print(f\"Average forest proximity score: {df_real['forest_proximity_score'].mean():.2f}\")\n",
    "print(f\"Average green space ratio: {df_real['green_space_ratio'].mean():.2f}\")\n",
    "\n",
    "# Visualize natural features data\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_real[natural_features_cols].boxplot()\n",
    "plt.title('Distribution of Natural Features Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Distance (km) / Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot comparing green space ratio to quality of life\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_real['green_space_ratio'], df_real['happiness_index'])\n",
    "plt.xlabel('Green Space Ratio')\n",
    "plt.ylabel('Happiness Index')\n",
    "plt.title('Correlation between Green Space and Happiness')\n",
    "for i, txt in enumerate(df_real['city_name']):\n",
    "    plt.annotate(txt, (df_real['green_space_ratio'].iloc[i], df_real['happiness_index'].iloc[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample city data\n",
    "raw_data = {\n",
    "    'city_name': ['New York', 'London', 'Tokyo', 'Paris', 'Singapore',\n",
    "                  'new york', 'LONDON', 'токио', 'paris ', 'Singapore'],\n",
    "    'country': ['USA', 'UK', 'Japan', 'France', 'Singapore',\n",
    "               'USA', 'UK', 'Japan', 'France', 'Singapore'],\n",
    "    'population': [8400000, 8900000, 37400000, 2100000, 5700000,\n",
    "                  np.nan, 8900000, 37400000, np.nan, 5700000],\n",
    "    'quality_of_life': [75.5, 82.3, 80.1, 85.2, 90.0,\n",
    "                       75.5, 82.3, 80.1, 85.2, np.nan],\n",
    "    'coordinates': ['40.7128,-74.0060', '51.5074,-0.1278', '35.6762,139.6503',\n",
    "                   '48.8566,2.3522', '1.3521,103.8198',\n",
    "                   '40.7128,-74.0060', '', '', '48.8566,2.3522', ''],\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "print(\"Raw data sample:\")\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0627fd",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Transformation\n",
    "\n",
    "Let's clean and transform our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89306238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_data(df):\n",
    "    \"\"\"Clean and transform city data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Standardize city names\n",
    "    df_clean['city_name'] = df_clean['city_name'].str.strip().str.title()\n",
    "    \n",
    "    # Clean country names\n",
    "    df_clean['country'] = df_clean['country'].str.strip()\n",
    "    \n",
    "    # Handle missing population data\n",
    "    df_clean['population'] = df_clean.groupby('city_name')['population'].transform(\n",
    "        lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # Extract coordinates\n",
    "    df_clean[['latitude', 'longitude']] = df_clean['coordinates'].str.split(',', expand=True)\n",
    "    df_clean['latitude'] = pd.to_numeric(df_clean['latitude'], errors='coerce')\n",
    "    df_clean['longitude'] = pd.to_numeric(df_clean['longitude'], errors='coerce')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=['city_name', 'country'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean the data\n",
    "df_cleaned = clean_city_data(df_raw)\n",
    "print(\"Cleaned data:\")\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547b71d",
   "metadata": {},
   "source": [
    "## 4. Data Validation\n",
    "\n",
    "Let's validate our cleaned data to ensure it meets our quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_city_data(df):\n",
    "    \"\"\"Validate cleaned city data and generate report\"\"\"\n",
    "    validation_results = {\n",
    "        'total_records': len(df),\n",
    "        'unique_cities': df['city_name'].nunique(),\n",
    "        'unique_countries': df['country'].nunique(),\n",
    "        'missing_coordinates': df[df['latitude'].isna() | df['longitude'].isna()].shape[0],\n",
    "        'missing_population': df['population'].isna().sum(),\n",
    "        'quality_score_range': f\"{df['quality_of_life'].min():.1f} - {df['quality_of_life'].max():.1f}\"\n",
    "    }\n",
    "    \n",
    "    # Print validation report\n",
    "    print(\"Data Validation Report:\")\n",
    "    for key, value in validation_results.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "    return validation_results\n",
    "\n",
    "# Validate the cleaned data\n",
    "validation_results = validate_city_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66dad5e",
   "metadata": {},
   "source": [
    "## 5. Database Loading\n",
    "\n",
    "Now let's load the cleaned and validated data into our PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_database(df, engine):\n",
    "    \"\"\"Load data to PostgreSQL with transaction management\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            # Create temporary table for upserting\n",
    "            df.to_sql('cities_temp', connection, if_exists='replace', index=False)\n",
    "            \n",
    "            # Perform upsert using a transaction\n",
    "            upsert_query = \"\"\"\n",
    "            INSERT INTO cities (city_name, country, latitude, longitude, population)\n",
    "            SELECT city_name, country, latitude, longitude, population\n",
    "            FROM cities_temp\n",
    "            ON CONFLICT (city_name, country)\n",
    "            DO UPDATE SET\n",
    "                latitude = EXCLUDED.latitude,\n",
    "                longitude = EXCLUDED.longitude,\n",
    "                population = EXCLUDED.population;\n",
    "            \"\"\"\n",
    "            \n",
    "            connection.execute(text(upsert_query))\n",
    "            logger.info(\"Successfully loaded data to database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load data to database\n",
    "load_to_database(df_cleaned, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6e551",
   "metadata": {},
   "source": [
    "## 6. Verify Data Loading\n",
    "\n",
    "Let's verify that our data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to verify loaded data\n",
    "verification_query = \"\"\"\n",
    "SELECT \n",
    "    city_name,\n",
    "    country,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    population\n",
    "FROM cities\n",
    "ORDER BY population DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "df_verified = pd.read_sql(verification_query, engine)\n",
    "print(\"Verified data in database:\")\n",
    "df_verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4e4ce",
   "metadata": {},
   "source": [
    "# City Data ETL Project\n",
    "\n",
    "This notebook demonstrates the process of collecting, transforming, and analyzing city data using Python and PostgreSQL. We'll explore various aspects of urban life including:\n",
    "- Cultural and social indicators\n",
    "- Economic metrics\n",
    "- Environmental factors\n",
    "- Quality of life measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454632fd",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all required libraries and set up our database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a635a",
   "metadata": {},
   "source": [
    "## 2. Create Database Schema\n",
    "\n",
    "Let's create our database schema using SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc660227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables SQL\n",
    "create_tables_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS cities (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    city_name VARCHAR(100) NOT NULL,\n",
    "    country VARCHAR(100) NOT NULL,\n",
    "    latitude FLOAT CHECK (latitude BETWEEN -90 AND 90),\n",
    "    longitude FLOAT CHECK (longitude BETWEEN -180 AND 180),\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    UNIQUE (city_name, country)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS city_metrics (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    city_id INTEGER REFERENCES cities(id),\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    english_proficiency_score FLOAT CHECK (english_proficiency_score BETWEEN 0 AND 100),\n",
    "    lgbt_acceptance_score FLOAT CHECK (lgbt_acceptance_score BETWEEN 0 AND 100),\n",
    "    cultural_events_per_capita FLOAT CHECK (cultural_events_per_capita >= 0),\n",
    "    education_level_score FLOAT CHECK (education_level_score BETWEEN 0 AND 100),\n",
    "    happiness_index FLOAT CHECK (happiness_index BETWEEN 0 AND 100),\n",
    "    sports_facilities_per_capita FLOAT CHECK (sports_facilities_per_capita >= 0),\n",
    "    health_index FLOAT CHECK (health_index BETWEEN 0 AND 100),\n",
    "    noise_pollution_level FLOAT CHECK (noise_pollution_level BETWEEN 0 AND 100),\n",
    "    air_quality_index FLOAT CHECK (air_quality_index BETWEEN 0 AND 500),\n",
    "    water_quality_score FLOAT,\n",
    "    economic_growth_rate FLOAT,\n",
    "    cost_of_living_index FLOAT,\n",
    "    average_salary FLOAT,\n",
    "    housing_price_index FLOAT,\n",
    "    rent_price_index FLOAT,\n",
    "    wind_speed_avg FLOAT,\n",
    "    sunny_days_per_year INTEGER,\n",
    "    distance_to_water FLOAT,\n",
    "    distance_to_mountains FLOAT,\n",
    "    forest_proximity_score FLOAT,\n",
    "    green_space_ratio FLOAT,\n",
    "    traffic_congestion_score FLOAT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute SQL to create tables\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_tables_sql))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887b28",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "\n",
    "We'll collect data from various sources. For this example, we'll use:\n",
    "- Numbeo API for cost of living and quality of life data\n",
    "- OpenWeatherMap for weather data\n",
    "- World Bank API for economic indicators\n",
    "\n",
    "Note: You'll need to sign up for API keys for these services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_numbeo_data(city, country):\n",
    "    \"\"\"Simulate fetching data from Numbeo API\"\"\"\n",
    "    # This is a mock function - you'll need to implement real API calls\n",
    "    return {\n",
    "        'cost_of_living_index': np.random.uniform(50, 150),\n",
    "        'rent_index': np.random.uniform(20, 100),\n",
    "        'health_care_index': np.random.uniform(40, 90),\n",
    "        'quality_of_life_index': np.random.uniform(60, 180)\n",
    "    }\n",
    "\n",
    "def fetch_weather_data(city, country):\n",
    "    \"\"\"Simulate fetching weather data\"\"\"\n",
    "    return {\n",
    "        'wind_speed_avg': np.random.uniform(2, 15),\n",
    "        'sunny_days': int(np.random.uniform(100, 300))\n",
    "    }\n",
    "\n",
    "# Sample cities\n",
    "sample_cities = [\n",
    "    {'name': 'New York', 'country': 'USA'},\n",
    "    {'name': 'London', 'country': 'UK'},\n",
    "    {'name': 'Tokyo', 'country': 'Japan'},\n",
    "    {'name': 'Berlin', 'country': 'Germany'},\n",
    "    {'name': 'Singapore', 'country': 'Singapore'}\n",
    "]\n",
    "\n",
    "# Collect data for each city\n",
    "city_data = []\n",
    "for city in sample_cities:\n",
    "    numbeo_data = fetch_numbeo_data(city['name'], city['country'])\n",
    "    weather_data = fetch_weather_data(city['name'], city['country'])\n",
    "    \n",
    "    city_data.append({\n",
    "        'city_name': city['name'],\n",
    "        'country': city['country'],\n",
    "        **numbeo_data,\n",
    "        **weather_data\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_raw = pd.DataFrame(city_data)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cc91b",
   "metadata": {},
   "source": [
    "## 4. Data Transformation\n",
    "\n",
    "Now let's clean and transform our collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4de1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_city_data(df):\n",
    "    \"\"\"Transform and clean the raw city data\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Add timestamp\n",
    "    df_clean['timestamp'] = datetime.now()\n",
    "    \n",
    "    # Normalize numerical columns to 0-100 scale\n",
    "    numerical_cols = ['cost_of_living_index', 'rent_index', 'health_care_index', 'quality_of_life_index']\n",
    "    for col in numerical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = 100 * (df_clean[col] - df_clean[col].min()) / (df_clean[col].max() - df_clean[col].min())\n",
    "    \n",
    "    # Round numerical values\n",
    "    df_clean = df_clean.round(2)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Transform the data\n",
    "df_transformed = transform_city_data(df_raw)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d995e",
   "metadata": {},
   "source": [
    "## 5. Load Data into Database\n",
    "\n",
    "Now we'll load our transformed data into PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cities(df, engine):\n",
    "    \"\"\"Load city data into the database with proper duplicate handling\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            # Create temporary table for upserting cities\n",
    "            cities_df = df[['city_name', 'country', 'latitude', 'longitude']].drop_duplicates()\n",
    "            cities_df.to_sql('cities_temp', connection, if_exists='replace', index=False)\n",
    "            \n",
    "            # Upsert cities using ON CONFLICT\n",
    "            upsert_query = \"\"\"\n",
    "                INSERT INTO cities (city_name, country, latitude, longitude)\n",
    "                SELECT city_name, country, latitude, longitude\n",
    "                FROM cities_temp\n",
    "                ON CONFLICT (city_name, country) \n",
    "                DO UPDATE SET\n",
    "                    latitude = EXCLUDED.latitude,\n",
    "                    longitude = EXCLUDED.longitude;\n",
    "            \"\"\"\n",
    "            connection.execute(text(upsert_query))\n",
    "            \n",
    "    # Get city IDs\n",
    "    city_ids = pd.read_sql(\n",
    "        'SELECT id, city_name FROM cities',\n",
    "        engine\n",
    "    ).set_index('city_name')['id'].to_dict()\n",
    "    \n",
    "    # Prepare metrics data\n",
    "    df['city_id'] = df['city_name'].map(city_ids)\n",
    "    metrics_cols = ['city_id', 'timestamp', 'cost_of_living_index', 'rent_index',\n",
    "                    'health_care_index', 'quality_of_life_index', 'wind_speed_avg',\n",
    "                    'sunny_days']\n",
    "    \n",
    "    # Insert metrics\n",
    "    df[metrics_cols].to_sql('city_metrics', engine, if_exists='append', index=False)\n",
    "\n",
    "# Load the transformed data\n",
    "load_cities(df_transformed, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befe80c",
   "metadata": {},
   "source": [
    "## 6. Analysis with SQL\n",
    "\n",
    "Let's analyze our data using SQL queries with window functions and joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex SQL query using window functions and joins\n",
    "analysis_query = \"\"\"\n",
    "WITH RankedCities AS (\n",
    "    SELECT \n",
    "        c.city_name,\n",
    "        c.country,\n",
    "        cm.cost_of_living_index,\n",
    "        cm.quality_of_life_index,\n",
    "        RANK() OVER (PARTITION BY c.country \n",
    "                     ORDER BY cm.quality_of_life_index DESC) as country_rank,\n",
    "        AVG(cm.cost_of_living_index) OVER (\n",
    "            PARTITION BY c.country\n",
    "        ) as avg_country_cost\n",
    "    FROM cities c\n",
    "    JOIN city_metrics cm ON c.id = cm.city_id\n",
    ")\n",
    "SELECT \n",
    "    city_name,\n",
    "    country,\n",
    "    cost_of_living_index,\n",
    "    quality_of_life_index,\n",
    "    country_rank,\n",
    "    avg_country_cost,\n",
    "    cost_of_living_index - avg_country_cost as cost_diff_from_avg\n",
    "FROM RankedCities\n",
    "ORDER BY quality_of_life_index DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "df_analysis = pd.read_sql(analysis_query, engine)\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b9e12",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Finally, let's create some visualizations of our analyzed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ef904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('seaborn')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Quality of Life vs Cost of Living\n",
    "sns.scatterplot(\n",
    "    data=df_analysis,\n",
    "    x='cost_of_living_index',\n",
    "    y='quality_of_life_index',\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Quality of Life vs Cost of Living')\n",
    "\n",
    "# Plot 2: Cost difference from country average\n",
    "sns.barplot(\n",
    "    data=df_analysis,\n",
    "    x='city_name',\n",
    "    y='cost_diff_from_avg',\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title('Cost of Living Difference from Country Average')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
