{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ef7c8",
   "metadata": {},
   "source": [
    "# City Geographic Data ETL Project\n",
    "\n",
    "This notebook demonstrates the ETL (Extract, Transform, Load) process for city geographic and quality-of-life data. We'll cover:\n",
    "- Setting up database connections\n",
    "- Loading and cleaning raw data\n",
    "- Transforming and validating data\n",
    "- Loading data into PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4843446",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import required libraries and configure our database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection with error handling\n",
    "try:\n",
    "    DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "    if not DATABASE_URL:\n",
    "        raise ValueError(\"DATABASE_URL environment variable is not set\")\n",
    "        \n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"SELECT 1\"))\n",
    "    logger.info(\"Successfully connected to the database\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Database connection error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c3517",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data\n",
    "\n",
    "Let's create some sample city data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample city data\n",
    "raw_data = {\n",
    "    'city_name': ['New York', 'London', 'Tokyo', 'Paris', 'Singapore',\n",
    "                  'new york', 'LONDON', 'токио', 'paris ', 'Singapore'],\n",
    "    'country': ['USA', 'UK', 'Japan', 'France', 'Singapore',\n",
    "               'USA', 'UK', 'Japan', 'France', 'Singapore'],\n",
    "    'population': [8400000, 8900000, 37400000, 2100000, 5700000,\n",
    "                  np.nan, 8900000, 37400000, np.nan, 5700000],\n",
    "    'quality_of_life': [75.5, 82.3, 80.1, 85.2, 90.0,\n",
    "                       75.5, 82.3, 80.1, 85.2, np.nan],\n",
    "    'coordinates': ['40.7128,-74.0060', '51.5074,-0.1278', '35.6762,139.6503',\n",
    "                   '48.8566,2.3522', '1.3521,103.8198',\n",
    "                   '40.7128,-74.0060', '', '', '48.8566,2.3522', ''],\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "print(\"Raw data sample:\")\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0627fd",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Transformation\n",
    "\n",
    "Let's clean and transform our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89306238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_data(df):\n",
    "    \"\"\"Clean and transform city data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Standardize city names\n",
    "    df_clean['city_name'] = df_clean['city_name'].str.strip().str.title()\n",
    "    \n",
    "    # Clean country names\n",
    "    df_clean['country'] = df_clean['country'].str.strip()\n",
    "    \n",
    "    # Handle missing population data\n",
    "    df_clean['population'] = df_clean.groupby('city_name')['population'].transform(\n",
    "        lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # Extract coordinates\n",
    "    df_clean[['latitude', 'longitude']] = df_clean['coordinates'].str.split(',', expand=True)\n",
    "    df_clean['latitude'] = pd.to_numeric(df_clean['latitude'], errors='coerce')\n",
    "    df_clean['longitude'] = pd.to_numeric(df_clean['longitude'], errors='coerce')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=['city_name', 'country'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean the data\n",
    "df_cleaned = clean_city_data(df_raw)\n",
    "print(\"Cleaned data:\")\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547b71d",
   "metadata": {},
   "source": [
    "## 4. Data Validation\n",
    "\n",
    "Let's validate our cleaned data to ensure it meets our quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_city_data(df):\n",
    "    \"\"\"Validate cleaned city data and generate report\"\"\"\n",
    "    validation_results = {\n",
    "        'total_records': len(df),\n",
    "        'unique_cities': df['city_name'].nunique(),\n",
    "        'unique_countries': df['country'].nunique(),\n",
    "        'missing_coordinates': df[df['latitude'].isna() | df['longitude'].isna()].shape[0],\n",
    "        'missing_population': df['population'].isna().sum(),\n",
    "        'quality_score_range': f\"{df['quality_of_life'].min():.1f} - {df['quality_of_life'].max():.1f}\"\n",
    "    }\n",
    "    \n",
    "    # Print validation report\n",
    "    print(\"Data Validation Report:\")\n",
    "    for key, value in validation_results.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "    return validation_results\n",
    "\n",
    "# Validate the cleaned data\n",
    "validation_results = validate_city_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66dad5e",
   "metadata": {},
   "source": [
    "## 5. Database Loading\n",
    "\n",
    "Now let's load the cleaned and validated data into our PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_database(df, engine):\n",
    "    \"\"\"Load data to PostgreSQL with transaction management\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            # Create temporary table for upserting\n",
    "            df.to_sql('cities_temp', connection, if_exists='replace', index=False)\n",
    "            \n",
    "            # Perform upsert using a transaction\n",
    "            upsert_query = \"\"\"\n",
    "            INSERT INTO cities (city_name, country, latitude, longitude, population)\n",
    "            SELECT city_name, country, latitude, longitude, population\n",
    "            FROM cities_temp\n",
    "            ON CONFLICT (city_name, country)\n",
    "            DO UPDATE SET\n",
    "                latitude = EXCLUDED.latitude,\n",
    "                longitude = EXCLUDED.longitude,\n",
    "                population = EXCLUDED.population;\n",
    "            \"\"\"\n",
    "            \n",
    "            connection.execute(text(upsert_query))\n",
    "            logger.info(\"Successfully loaded data to database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load data to database\n",
    "load_to_database(df_cleaned, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6e551",
   "metadata": {},
   "source": [
    "## 6. Verify Data Loading\n",
    "\n",
    "Let's verify that our data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to verify loaded data\n",
    "verification_query = \"\"\"\n",
    "SELECT \n",
    "    city_name,\n",
    "    country,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    population\n",
    "FROM cities\n",
    "ORDER BY population DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and display results\n",
    "df_verified = pd.read_sql(verification_query, engine)\n",
    "print(\"Verified data in database:\")\n",
    "df_verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4e4ce",
   "metadata": {},
   "source": [
    "# City Data ETL Project\n",
    "\n",
    "This notebook demonstrates the process of collecting, transforming, and analyzing city data using Python and PostgreSQL. We'll explore various aspects of urban life including:\n",
    "- Cultural and social indicators\n",
    "- Economic metrics\n",
    "- Environmental factors\n",
    "- Quality of life measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454632fd",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all required libraries and set up our database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a635a",
   "metadata": {},
   "source": [
    "## 2. Create Database Schema\n",
    "\n",
    "Let's create our database schema using SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc660227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables SQL\n",
    "create_tables_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS cities (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    city_name VARCHAR(100) NOT NULL,\n",
    "    country VARCHAR(100) NOT NULL,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS city_metrics (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    city_id INTEGER REFERENCES cities(id),\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    english_proficiency_score FLOAT,\n",
    "    lgbt_acceptance_score FLOAT,\n",
    "    cultural_events_per_capita FLOAT,\n",
    "    education_level_score FLOAT,\n",
    "    happiness_index FLOAT,\n",
    "    sports_facilities_per_capita FLOAT,\n",
    "    health_index FLOAT,\n",
    "    noise_pollution_level FLOAT,\n",
    "    air_quality_index FLOAT,\n",
    "    water_quality_score FLOAT,\n",
    "    economic_growth_rate FLOAT,\n",
    "    cost_of_living_index FLOAT,\n",
    "    average_salary FLOAT,\n",
    "    housing_price_index FLOAT,\n",
    "    rent_price_index FLOAT,\n",
    "    wind_speed_avg FLOAT,\n",
    "    sunny_days_per_year INTEGER,\n",
    "    distance_to_water FLOAT,\n",
    "    distance_to_mountains FLOAT,\n",
    "    forest_proximity_score FLOAT,\n",
    "    green_space_ratio FLOAT,\n",
    "    traffic_congestion_score FLOAT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute SQL to create tables\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_tables_sql))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887b28",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "\n",
    "We'll collect data from various sources. For this example, we'll use:\n",
    "- Numbeo API for cost of living and quality of life data\n",
    "- OpenWeatherMap for weather data\n",
    "- World Bank API for economic indicators\n",
    "\n",
    "Note: You'll need to sign up for API keys for these services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_numbeo_data(city, country):\n",
    "    \"\"\"Simulate fetching data from Numbeo API\"\"\"\n",
    "    # This is a mock function - you'll need to implement real API calls\n",
    "    return {\n",
    "        'cost_of_living_index': np.random.uniform(50, 150),\n",
    "        'rent_index': np.random.uniform(20, 100),\n",
    "        'health_care_index': np.random.uniform(40, 90),\n",
    "        'quality_of_life_index': np.random.uniform(60, 180)\n",
    "    }\n",
    "\n",
    "def fetch_weather_data(city, country):\n",
    "    \"\"\"Simulate fetching weather data\"\"\"\n",
    "    return {\n",
    "        'wind_speed_avg': np.random.uniform(2, 15),\n",
    "        'sunny_days': int(np.random.uniform(100, 300))\n",
    "    }\n",
    "\n",
    "# Sample cities\n",
    "sample_cities = [\n",
    "    {'name': 'New York', 'country': 'USA'},\n",
    "    {'name': 'London', 'country': 'UK'},\n",
    "    {'name': 'Tokyo', 'country': 'Japan'},\n",
    "    {'name': 'Berlin', 'country': 'Germany'},\n",
    "    {'name': 'Singapore', 'country': 'Singapore'}\n",
    "]\n",
    "\n",
    "# Collect data for each city\n",
    "city_data = []\n",
    "for city in sample_cities:\n",
    "    numbeo_data = fetch_numbeo_data(city['name'], city['country'])\n",
    "    weather_data = fetch_weather_data(city['name'], city['country'])\n",
    "    \n",
    "    city_data.append({\n",
    "        'city_name': city['name'],\n",
    "        'country': city['country'],\n",
    "        **numbeo_data,\n",
    "        **weather_data\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_raw = pd.DataFrame(city_data)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cc91b",
   "metadata": {},
   "source": [
    "## 4. Data Transformation\n",
    "\n",
    "Now let's clean and transform our collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4de1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_city_data(df):\n",
    "    \"\"\"Transform and clean the raw city data\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Add timestamp\n",
    "    df_clean['timestamp'] = datetime.now()\n",
    "    \n",
    "    # Normalize numerical columns to 0-100 scale\n",
    "    numerical_cols = ['cost_of_living_index', 'rent_index', 'health_care_index', 'quality_of_life_index']\n",
    "    for col in numerical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = 100 * (df_clean[col] - df_clean[col].min()) / (df_clean[col].max() - df_clean[col].min())\n",
    "    \n",
    "    # Round numerical values\n",
    "    df_clean = df_clean.round(2)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Transform the data\n",
    "df_transformed = transform_city_data(df_raw)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d995e",
   "metadata": {},
   "source": [
    "## 5. Load Data into Database\n",
    "\n",
    "Now we'll load our transformed data into PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cities(df, engine):\n",
    "    \"\"\"Load city data into the database\"\"\"\n",
    "    # Insert cities\n",
    "    cities_df = df[['city_name', 'country']].drop_duplicates()\n",
    "    cities_df.to_sql('cities', engine, if_exists='append', index=False)\n",
    "    \n",
    "    # Get city IDs\n",
    "    city_ids = pd.read_sql(\n",
    "        'SELECT id, city_name FROM cities',\n",
    "        engine\n",
    "    ).set_index('city_name')['id'].to_dict()\n",
    "    \n",
    "    # Prepare metrics data\n",
    "    df['city_id'] = df['city_name'].map(city_ids)\n",
    "    metrics_cols = ['city_id', 'timestamp', 'cost_of_living_index', 'rent_index',\n",
    "                    'health_care_index', 'quality_of_life_index', 'wind_speed_avg',\n",
    "                    'sunny_days']\n",
    "    \n",
    "    # Insert metrics\n",
    "    df[metrics_cols].to_sql('city_metrics', engine, if_exists='append', index=False)\n",
    "\n",
    "# Load the transformed data\n",
    "load_cities(df_transformed, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befe80c",
   "metadata": {},
   "source": [
    "## 6. Analysis with SQL\n",
    "\n",
    "Let's analyze our data using SQL queries with window functions and joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex SQL query using window functions and joins\n",
    "analysis_query = \"\"\"\n",
    "WITH RankedCities AS (\n",
    "    SELECT \n",
    "        c.city_name,\n",
    "        c.country,\n",
    "        cm.cost_of_living_index,\n",
    "        cm.quality_of_life_index,\n",
    "        RANK() OVER (PARTITION BY c.country \n",
    "                     ORDER BY cm.quality_of_life_index DESC) as country_rank,\n",
    "        AVG(cm.cost_of_living_index) OVER (\n",
    "            PARTITION BY c.country\n",
    "        ) as avg_country_cost\n",
    "    FROM cities c\n",
    "    JOIN city_metrics cm ON c.id = cm.city_id\n",
    ")\n",
    "SELECT \n",
    "    city_name,\n",
    "    country,\n",
    "    cost_of_living_index,\n",
    "    quality_of_life_index,\n",
    "    country_rank,\n",
    "    avg_country_cost,\n",
    "    cost_of_living_index - avg_country_cost as cost_diff_from_avg\n",
    "FROM RankedCities\n",
    "ORDER BY quality_of_life_index DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "df_analysis = pd.read_sql(analysis_query, engine)\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b9e12",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Finally, let's create some visualizations of our analyzed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ef904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('seaborn')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Quality of Life vs Cost of Living\n",
    "sns.scatterplot(\n",
    "    data=df_analysis,\n",
    "    x='cost_of_living_index',\n",
    "    y='quality_of_life_index',\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Quality of Life vs Cost of Living')\n",
    "\n",
    "# Plot 2: Cost difference from country average\n",
    "sns.barplot(\n",
    "    data=df_analysis,\n",
    "    x='city_name',\n",
    "    y='cost_diff_from_avg',\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title('Cost of Living Difference from Country Average')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
